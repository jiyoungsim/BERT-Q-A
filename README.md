# BERT-Q-A
Modified BERT for Q-A task

+ Reference
    + [Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding.arXiv preprint arXiv:1810.04805 (2018).](https://arxiv.org/abs/1810.04805)
    + [Ryan McDonald, Yichun Ding, and Ion Androutsopoulos. 2018. Deep RelevanceRanking using Enhanced Document-Query Interactions. In EMNLP.](https://arxiv.org/abs/1809.01682)
