# BERT-Q-A
Modified BERT for Q-A task

+ Reference
    + [Ryan McDonald, Georgios-Ioannis Brokos, Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-Query Interactions. In EMNLP](https://arxiv.org/abs/1809.01682)
    + [Sean MacAvaney, Andrew Yates, Arman Cohan, Nazli Goharian. 2019. CEDR: Contextualized Embeddings for Document Ranking. In SIGIR](https://arxiv.org/abs/1904.07094)
    + [Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking.	arXiv:1904.07531](https://arxiv.org/abs/1904.07531v4)
    + [Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding.arXiv preprint arXiv:1810.04805](https://arxiv.org/abs/1810.04805)
    + [TensorFlow code and pre-trained models for BERT from Google Research](https://github.com/google-research/bert)
    + [Ryan McDonald, Yichun Ding, and Ion Androutsopoulos. 2018. Deep RelevanceRanking using Enhanced Document-Query Interactions. In EMNLP.](https://arxiv.org/abs/1809.01682)
